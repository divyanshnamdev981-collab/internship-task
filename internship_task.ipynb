{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "nk8Qxy6q-bnJ",
        "outputId": "5188bf1d-c3e6-4bf0-daf2-022368aecd58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❌ Failed to fetch data:\n",
            "Status Code: 401\n",
            "Response: {'cod': 401, 'message': 'Invalid API key. Please see https://openweathermap.org/faq#error401 for more info.'}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'list'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3291622151.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mhumidities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mdt_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dt_txt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'main'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'list'"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIGURATION\n",
        "# -----------------------------\n",
        "API_KEY = \"YOUR_API_KEY\"  # <-- Replace this\n",
        "CITY = \"Mumbai\"\n",
        "UNITS = \"metric\"\n",
        "URL = f\"http://api.openweathermap.org/data/2.5/forecast?q={CITY}&appid={API_KEY}&units={UNITS}\"\n",
        "\n",
        "# -----------------------------\n",
        "# FETCH WEATHER DATA\n",
        "# -----------------------------\n",
        "response = requests.get(URL)\n",
        "data = response.json()\n",
        "\n",
        "# -----------------------------\n",
        "# CHECK FOR ERRORS\n",
        "# -----------------------------\n",
        "if response.status_code != 200 or 'list' not in data:\n",
        "    print(\"\\n❌ Failed to fetch data:\")\n",
        "    print(\"Status Code:\", response.status_code)\n",
        "    print(\"Response:\", data)\n",
        "    exit()\n",
        "\n",
        "# -----------------------------\n",
        "# PARSE WEATHER DATA\n",
        "# -----------------------------\n",
        "dates = []\n",
        "temperatures = []\n",
        "humidities = []\n",
        "\n",
        "for entry in data['list']:\n",
        "    dt_txt = entry['dt_txt']\n",
        "    temp = entry['main']['temp']\n",
        "    humidity = entry['main']['humidity']\n",
        "\n",
        "    dates.append(datetime.strptime(dt_txt, '%Y-%m-%d %H:%M:%S'))\n",
        "    temperatures.append(temp)\n",
        "    humidities.append(humidity)\n",
        "\n",
        "# -----------------------------\n",
        "# VISUALIZATION\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.lineplot(x=dates, y=temperatures, marker='o', color='tomato')\n",
        "plt.title(f\"5-Day Temperature Forecast for {CITY}\", fontsize=16)\n",
        "plt.xlabel(\"Date & Time\")\n",
        "plt.ylabel(\"Temperature (°C)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from fpdf import FPDF\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# Analysis\n",
        "summary = df.groupby(\"Department\")[\"Salary\"].agg([\"mean\", \"max\", \"min\", \"count\"]).reset_index()\n",
        "\n",
        "# PDF Setup\n",
        "class PDF(FPDF):\n",
        "    def header(self):\n",
        "        self.set_font(\"Arial\", \"B\", 14)\n",
        "        self.cell(200, 10, \"Employee Salary Report\", ln=True, align=\"C\")\n",
        "\n",
        "    def footer(self):\n",
        "        self.set_y(-15)\n",
        "        self.set_font(\"Arial\", \"I\", 8)\n",
        "        self.cell(0, 10, f\"Page {self.page_no()}\", align=\"C\")\n",
        "\n",
        "    def chapter_title(self, title):\n",
        "        self.set_font(\"Arial\", \"B\", 12)\n",
        "        self.cell(0, 10, title, ln=True)\n",
        "\n",
        "    def add_summary_table(self, summary_df):\n",
        "        self.set_font(\"Arial\", \"\", 11)\n",
        "        self.cell(0, 10, \"\", ln=True)  # spacing\n",
        "        col_widths = [40, 30, 30, 30, 30]\n",
        "        headers = [\"Department\", \"Avg Salary\", \"Max Salary\", \"Min Salary\", \"Count\"]\n",
        "\n",
        "        # Header Row\n",
        "        for i, header in enumerate(headers):\n",
        "            self.cell(col_widths[i], 10, header, border=1, align=\"C\")\n",
        "        self.ln()\n",
        "\n",
        "        # Data Rows\n",
        "        for row in summary_df.itertuples(index=False):\n",
        "            self.cell(col_widths[0], 10, str(row[0]), border=1)\n",
        "            self.cell(col_widths[1], 10, f\"{row[1]:.2f}\", border=1)\n",
        "            self.cell(col_widths[2], 10, f\"{row[2]}\", border=1)\n",
        "            self.cell(col_widths[3], 10, f\"{row[3]}\", border=1)\n",
        "            self.cell(col_widths[4], 10, f\"{row[4]}\", border=1)\n",
        "            self.ln()\n",
        "\n",
        "# Create PDF\n",
        "pdf = PDF()\n",
        "pdf.add_page()\n",
        "pdf.chapter_title(\"Department-wise Salary Summary\")\n",
        "pdf.add_summary_table(summary)\n",
        "pdf.output(\"Employee_Salary_Report.pdf\")\n",
        "\n",
        "print(\"✅ PDF report generated: Employee_Salary_Report.pdf\")\n"
      ],
      "metadata": {
        "id": "STUUFFFFBBRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample corpus (You can expand this or load from a file)\n",
        "corpus = \"\"\"\n",
        "Hello, how can I help you?\n",
        "What is your name?\n",
        "I am a chatbot built using Python.\n",
        "Tell me about natural language processing.\n",
        "NLP stands for Natural Language Processing.\n",
        "It is a field of AI that deals with human language.\n",
        "What can you do?\n",
        "I can answer questions, perform small talk, and help you with basic queries.\n",
        "Goodbye!\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize sentences\n",
        "sent_tokens = nltk.sent_tokenize(corpus.lower())\n",
        "\n",
        "# Lemmatizer\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
        "\n",
        "# GREETING INPUTS AND RESPONSES\n",
        "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\")\n",
        "GREETING_RESPONSES = [\"Hi there!\", \"Hey!\", \"Hello!\", \"Greetings!\", \"Hi! How can I assist?\"]\n",
        "\n",
        "def greet(sentence):\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)\n",
        "\n",
        "# RESPONSE GENERATION\n",
        "def generate_response(user_input):\n",
        "    robo_response = ''\n",
        "    sent_tokens.append(user_input)\n",
        "\n",
        "    # Vectorization\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "\n",
        "    # Compute similarity\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf[:-1])\n",
        "    idx = np.argmax(vals)\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "\n",
        "    # Check confidence\n",
        "    if req_tfidf == 0:\n",
        "        robo_response = \"I'm sorry, I didn't understand that.\"\n",
        "    else:\n",
        "        robo_response = sent_tokens[idx]\n",
        "\n",
        "    sent_tokens.pop()  # remove the last input to avoid growth\n",
        "    return robo_response\n",
        "\n",
        "# MAIN CHAT LOOP\n",
        "def chatbot():\n",
        "    print(\"BOT: Hello! I am your chatbot. Type 'bye' to exit.\")\n",
        "    while True:\n",
        "        user_input = input(\"YOU: \").lower()\n",
        "        if user_input in ['bye', 'exit', 'quit']:\n",
        "            print(\"BOT: Goodbye! Have a great day.\")\n",
        "            break\n",
        "        elif greet(user_input) is not None:\n",
        "            print(\"BOT:\", greet(user_input))\n",
        "        else:\n",
        "            print(\"BOT:\", generate_response(user_input))\n",
        "\n",
        "# Run the chatbot\n",
        "if __name__ == \"__main__\":\n",
        "    chatbot()\n"
      ],
      "metadata": {
        "id": "7Cd0_vTGBy1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Hello, how can I help you?\n",
        "What is your name?\n",
        "I am a chatbot built using Python.\n",
        "Tell me about natural language processing.\n",
        "NLP stands for Natural Language Processing.\n",
        "It is a field of AI that deals with human language.\n",
        "What can you do?\n",
        "I can answer questions, perform small talk, and help you with basic queries.\n",
        "Goodbye!\n"
      ],
      "metadata": {
        "id": "Qgz3-e-sCiQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK - 4 : MACHINE LEARNING MODEL IMPLEMENTATION\n",
        "# Example: Spam Email Detection using Scikit-learn\n",
        "\n",
        "# Step 1: Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Step 2: Load Dataset\n",
        "# Using a sample dataset (you can replace with any spam dataset e.g. from Kaggle)\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/justmarkham/scikit-learn-videos/master/data/sms.tsv\",\n",
        "                   sep='\\t', names=['label', 'message'])\n",
        "\n",
        "print(\"Dataset Sample:\")\n",
        "print(data.head())\n",
        "\n",
        "# Step 3: Preprocess Data\n",
        "X = data['message']\n",
        "y = data['label'].map({'ham': 0, 'spam': 1})   # Convert labels: ham=0, spam=1\n",
        "\n",
        "# Convert text into numerical vectors\n",
        "vectorizer = CountVectorizer()\n",
        "X_vectorized = vectorizer.fit_transform(X)\n",
        "\n",
        "# Step 4: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 5: Train Model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluation\n",
        "print(\"\\nModel Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 8: Test with custom input\n",
        "sample_msg = [\"Congratulations! You've won a free ticket, claim now!\",\n",
        "              \"Hey, are we meeting tomorrow at 5?\"]\n",
        "\n",
        "sample_msg_vec = vectorizer.transform(sample_msg)\n",
        "pred = model.predict(sample_msg_vec)\n",
        "\n",
        "print(\"\\nSample Predictions:\")\n",
        "for msg, label in zip(sample_msg, pred):\n",
        "    print(f\"Message: {msg} --> {'SPAM' if label==1 else 'HAM'}\")\n"
      ],
      "metadata": {
        "id": "_z94WdzZC7Fo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}